{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "59b334e2",
   "metadata": {},
   "source": [
    "## 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2447b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from PIL import Image, ImageFile\n",
    "import albumentations as A\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from tqdm import tqdm\n",
    "\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "print(\"‚úì All libraries imported successfully!\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7193c203",
   "metadata": {},
   "source": [
    "## 2. Configuration\n",
    "\n",
    "Set all paths and hyperparameters in one place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db9378b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    # Dataset paths\n",
    "    TRAIN_DIR = \"Dataset/train\"\n",
    "    VAL_DIR = \"Dataset/val\"\n",
    "    \n",
    "    # Image dimensions\n",
    "    IMAGE_HEIGHT = 40\n",
    "    IMAGE_WIDTH = 150\n",
    "    \n",
    "    # Training settings\n",
    "    BATCH_SIZE = 32\n",
    "    NUM_WORKERS = 4\n",
    "    EPOCHS = 10\n",
    "    LEARNING_RATE = 1e-3\n",
    "    \n",
    "    # Device\n",
    "    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(f\"Device: {Config.DEVICE}\")\n",
    "print(f\"Image size: {Config.IMAGE_HEIGHT} x {Config.IMAGE_WIDTH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e32de33c",
   "metadata": {},
   "source": [
    "## 3. Dataset Class\n",
    "\n",
    "**What it does:** Loads images, applies preprocessing, and returns tensors.\n",
    "\n",
    "**Preprocessing steps:**\n",
    "1. Resize to 40x150 (height x width)\n",
    "2. Normalize with ImageNet mean/std\n",
    "3. Transpose to (Channels, Height, Width)\n",
    "4. Convert to PyTorch tensor\n",
    "\n",
    "**Example:**\n",
    "```python\n",
    "# Input: \"8AE5T.jpg\" file\n",
    "# Output: \n",
    "#   - images: tensor of shape (3, 40, 150)\n",
    "#   - targets: tensor like [9, 11, 5, 20, 6] (encoded characters)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebc44bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassificationDataset(Dataset):\n",
    "    def __init__(self, image_paths, labels):\n",
    "        self.image_paths = image_paths\n",
    "        self.labels = labels\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Load and convert image to RGB\n",
    "        image = Image.open(self.image_paths[idx]).convert('RGB')\n",
    "        image = np.array(image)\n",
    "        \n",
    "        # Apply augmentations\n",
    "        augmented = self.transform(image=image)\n",
    "        image = augmented['image']\n",
    "        \n",
    "        # Transpose for PyTorch: (H,W,C) -> (C,H,W)\n",
    "        image = np.transpose(image, (2, 0, 1)).astype(np.float32)\n",
    "        \n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        return {\n",
    "            'images': torch.tensor(image, dtype=torch.float),\n",
    "            'targets': torch.tensor(label, dtype=torch.long),\n",
    "            'targets_len': torch.tensor(len(label), dtype=torch.long)\n",
    "        }\n",
    "    \n",
    "    def transform(self, image):\n",
    "        # Resize to model input size\n",
    "        resize = A.Resize(40, 150, always_apply=True)\n",
    "        # Normalize with ImageNet stats\n",
    "        normalize = A.Normalize(always_apply=True)\n",
    "        transform = A.Compose([resize, normalize])\n",
    "        return transform(image=image)\n",
    "\n",
    "print(\"‚úì Dataset class defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ae0a3bf",
   "metadata": {},
   "source": [
    "## üñºÔ∏è Dataset Class Explanation\n",
    "\n",
    "The `ClassificationDataset` loads and preprocesses images for the model.\n",
    "\n",
    "**Preprocessing Pipeline:**\n",
    "1. **Load Image**: Read CAPTCHA from disk as RGB\n",
    "2. **Resize**: Scale to 40√ó150 (height√ówidth) - landscape orientation\n",
    "3. **Normalize**: Apply ImageNet statistics (mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "4. **Transpose**: Convert (H,W,C) ‚Üí (C,H,W) for PyTorch\n",
    "\n",
    "**Example Data Flow:**\n",
    "```\n",
    "Input:  \"8AE5T.jpg\" (original size, maybe 200√ó80)\n",
    "Resize: (40, 150, 3) numpy array\n",
    "Normalize: Values scaled to ~[-2, 2] range\n",
    "Transpose: (3, 40, 150) tensor ready for model\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e62647e",
   "metadata": {},
   "source": [
    "## 4. Model Architecture\n",
    "\n",
    "**CNN + GRU with CTC Loss**\n",
    "\n",
    "**Structure:**\n",
    "- **Conv layers:** Extract visual features from images\n",
    "- **GRU layers:** Process features as a sequence (left to right)\n",
    "- **Classifier:** Predict character at each position\n",
    "- **CTC Loss:** Handle variable-length sequences without alignment\n",
    "\n",
    "**Dimensions transformation:**\n",
    "```\n",
    "Input:  (batch, 3, 40, 150)      # RGB image\n",
    "Conv:   (batch, 64, 10, 37)      # After convolution + pooling\n",
    "Permute:(batch, 37, 64, 10)      # Treat width as sequence\n",
    "Flatten:(batch, 37, 640)         # Flatten spatial dimensions\n",
    "Linear: (batch, 37, 64)          # Project to feature space\n",
    "GRU:    (batch, 37, 64)          # Bidirectional GRU\n",
    "Output: (37, batch, num_chars+1) # CTC format\n",
    "```\n",
    "\n",
    "**Why this works:**\n",
    "- CNN captures character shapes and edges\n",
    "- GRU learns sequence patterns (common character combinations)\n",
    "- CTC allows model to predict without knowing exact character positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d2c3aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CaptchaModel(nn.Module):\n",
    "    def __init__(self, num_characters):\n",
    "        super(CaptchaModel, self).__init__()\n",
    "        \n",
    "        # CNN layers for feature extraction\n",
    "        self.conv1 = nn.Conv2d(3, 128, kernel_size=(3, 3), padding=(1, 1))\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=(2, 2))\n",
    "        self.conv2 = nn.Conv2d(128, 64, kernel_size=(3, 3), padding=(1, 1))\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=(2, 2))\n",
    "        \n",
    "        # Bidirectional GRU for sequence modeling\n",
    "        self.gru = nn.GRU(640, 32, num_layers=2, bidirectional=True, dropout=0.25, batch_first=True)\n",
    "        \n",
    "        # Output layer (+1 for CTC blank token)\n",
    "        self.output = nn.Linear(64, num_characters + 1)\n",
    "        \n",
    "    def forward(self, images, targets=None, target_lengths=None):\n",
    "        bs = images.size(0)  # batch size\n",
    "        \n",
    "        # CNN feature extraction\n",
    "        x = F.relu(self.conv1(images))\n",
    "        x = self.pool1(x)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.pool2(x)\n",
    "        \n",
    "        # Reshape for sequence processing\n",
    "        x = x.permute(0, 3, 1, 2)  # (batch, width, height, channels)\n",
    "        x = x.view(bs, x.size(1), -1)  # (batch, sequence_len, features)\n",
    "        \n",
    "        # GRU sequence modeling\n",
    "        x, _ = self.gru(x)\n",
    "        \n",
    "        # Character predictions\n",
    "        x = self.output(x)\n",
    "        x = x.permute(1, 0, 2)  # (sequence_len, batch, num_classes) for CTC\n",
    "        \n",
    "        # Calculate loss if training\n",
    "        if targets is not None:\n",
    "            log_probs = F.log_softmax(x, dim=2)\n",
    "            input_lengths = torch.full((bs,), x.size(0), dtype=torch.long)\n",
    "            \n",
    "            loss = nn.CTCLoss(blank=0)(log_probs, targets, input_lengths, target_lengths)\n",
    "            return x, loss\n",
    "        \n",
    "        return x, None\n",
    "\n",
    "print(\"‚úì Model architecture defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee75ac2e",
   "metadata": {},
   "source": [
    "## üß† Model Architecture - Deep Dive\n",
    "\n",
    "### Forward Pass: Complete Dimension Transformation\n",
    "\n",
    "**Input:**\n",
    "- `images`: (batch=32, channels=3, height=40, width=150)\n",
    "- `targets`: (batch=32, seq_len=5) - e.g., [[9,11,5,20,6], [12,3,14,8,19], ...]\n",
    "- `target_lengths`: (batch=32,) - all values = 5\n",
    "\n",
    "**Step-by-Step Process:**\n",
    "\n",
    "```\n",
    "1. First Convolution Block\n",
    "   Input:  (32, 3, 40, 150)\n",
    "   ‚Üì Conv2d(3‚Üí128, kernel=3√ó3, padding=1)\n",
    "   (32, 128, 40, 150)  # 128 feature maps, same spatial size\n",
    "   ‚Üì ReLU + MaxPool2d(2√ó2)\n",
    "   (32, 128, 20, 75)   # Halved: height/2, width/2\n",
    "\n",
    "2. Second Convolution Block\n",
    "   (32, 128, 20, 75)\n",
    "   ‚Üì Conv2d(128‚Üí64, kernel=3√ó3, padding=1)\n",
    "   (32, 64, 20, 75)\n",
    "   ‚Üì ReLU + MaxPool2d(2√ó2)\n",
    "   (32, 64, 10, 37)    # Halved again: 20/2=10, 75/2=37\n",
    "\n",
    "3. Reshape for Sequence Processing\n",
    "   (32, 64, 10, 37)\n",
    "   ‚Üì permute(0, 3, 1, 2) - Treat width as sequence\n",
    "   (32, 37, 64, 10)    # 37 positions along width\n",
    "   ‚Üì view(batch, seq_len, -1) - Flatten spatial features\n",
    "   (32, 37, 640)       # 640 = 64 channels √ó 10 height\n",
    "\n",
    "4. Bidirectional GRU\n",
    "   Input: (32, 37, 640)\n",
    "   ‚Üì 2-layer BiGRU(hidden=32)\n",
    "   Output: (32, 37, 64)  # 64 = 32 forward + 32 backward\n",
    "   \n",
    "   What happens:\n",
    "   - Forward GRU: Reads sequence left‚Üíright (context from past)\n",
    "   - Backward GRU: Reads sequence right‚Üíleft (context from future)\n",
    "   - Concatenates: Each position gets 64 features\n",
    "\n",
    "5. Classification Layer\n",
    "   (32, 37, 64)\n",
    "   ‚Üì Linear(64 ‚Üí num_chars+1)\n",
    "   (32, 37, 37)        # 36 chars + 1 blank = 37 classes\n",
    "   ‚Üì permute(1, 0, 2) - CTC expects seq_len first\n",
    "   (37, 32, 37)        # Final output\n",
    "\n",
    "6. CTC Loss Calculation\n",
    "   Log probabilities: (37, 32, 37)\n",
    "   Targets: (32, 5) - e.g., [[9,11,5,20,6], ...]\n",
    "   \n",
    "   CTC aligns 37 predictions with 5 target characters\n",
    "   Allows for: \"AA_BB_CC_DD_EE____\" ‚Üí \"ABCDE\"\n",
    "   Where _ = blank token (index 0)\n",
    "```\n",
    "\n",
    "**Output:**\n",
    "- `logits`: (37, 32, 37) - Raw predictions for each of 37 positions\n",
    "- `loss`: Scalar - CTC loss value for optimization\n",
    "\n",
    "**Why This Architecture?**\n",
    "- **CNN**: Extracts visual features (edges, curves, character shapes)\n",
    "- **Sequence length 37**: Sufficient positions for 5-character captchas with blanks\n",
    "- **BiGRU**: Captures dependencies (e.g., \"Q\" often followed by \"U\")\n",
    "- **CTC Loss**: No need to know exact character positions in image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1fc089b",
   "metadata": {},
   "source": [
    "## 5. Training Functions\n",
    "\n",
    "**train_fn:** One epoch of training\n",
    "- Forward pass: get predictions\n",
    "- Calculate CTC loss\n",
    "- Backward pass: compute gradients\n",
    "- Update weights\n",
    "\n",
    "**eval_fn:** Validation\n",
    "- Forward pass only (no gradient computation)\n",
    "- Calculate validation loss\n",
    "- Return predictions for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d989083",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fn(model, data_loader, optimizer, device):\n",
    "    model.train()\n",
    "    fin_loss = 0\n",
    "    \n",
    "    for data in tqdm(data_loader, desc=\"Training\", leave=False):\n",
    "        images = data['images'].to(device)\n",
    "        targets = data['targets'].to(device)\n",
    "        target_lengths = data['targets_len'].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        logits, loss = model(images, targets, target_lengths)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        fin_loss += loss.item()\n",
    "    \n",
    "    return fin_loss / len(data_loader)\n",
    "\n",
    "\n",
    "def eval_fn(model, data_loader, device):\n",
    "    model.eval()\n",
    "    fin_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data in tqdm(data_loader, desc=\"Validating\", leave=False):\n",
    "            images = data['images'].to(device)\n",
    "            targets = data['targets'].to(device)\n",
    "            target_lengths = data['targets_len'].to(device)\n",
    "            \n",
    "            logits, loss = model(images, targets, target_lengths)\n",
    "            fin_loss += loss.item()\n",
    "    \n",
    "    return fin_loss / len(data_loader)\n",
    "\n",
    "print(\"‚úì Training functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bb5d0d5",
   "metadata": {},
   "source": [
    "## üîÑ Training & Evaluation - Deep Dive\n",
    "\n",
    "### `train_fn()` - One Training Epoch\n",
    "\n",
    "**Input:**\n",
    "- `model`: Neural network\n",
    "- `data_loader`: Yields batches of (images, targets, lengths)\n",
    "- `optimizer`: Adam optimizer (stores gradients and updates weights)\n",
    "- `device`: 'cuda' or 'cpu'\n",
    "\n",
    "**Process (Per Batch):**\n",
    "\n",
    "```\n",
    "1. Forward Pass\n",
    "   images (32, 3, 40, 150) ‚Üí model ‚Üí logits (37, 32, 37)\n",
    "   CTC Loss computes alignment between predictions and targets\n",
    "\n",
    "2. optimizer.zero_grad()\n",
    "   Reset all gradients to zero\n",
    "   Why? PyTorch accumulates gradients by default\n",
    "   Without this: gradients from previous batches interfere\n",
    "\n",
    "3. loss.backward()\n",
    "   Compute ‚àÇloss/‚àÇweight for EVERY parameter\n",
    "   Uses chain rule through entire network:\n",
    "   - Output layer ‚Üí GRU ‚Üí Conv2 ‚Üí Conv1\n",
    "   Magic of automatic differentiation!\n",
    "\n",
    "4. optimizer.step()\n",
    "   Update weights: w_new = w_old - lr √ó gradient\n",
    "   Adam is smart: adjusts learning rate per parameter\n",
    "   Frequently updated params ‚Üí smaller steps\n",
    "   Rarely updated params ‚Üí larger steps\n",
    "```\n",
    "\n",
    "**Output:**\n",
    "- Average loss across all batches\n",
    "- Lower loss = better predictions\n",
    "\n",
    "### `eval_fn()` - Validation\n",
    "\n",
    "**Key Difference: `torch.no_grad()`**\n",
    "\n",
    "```python\n",
    "with torch.no_grad():\n",
    "    # Forward pass only\n",
    "```\n",
    "\n",
    "**What this does:**\n",
    "1. **Disables gradient computation**\n",
    "   - Saves memory (no need to store intermediate activations)\n",
    "   - Faster computation (skips gradient graph building)\n",
    "\n",
    "2. **Evaluation mode (`model.eval()`)**\n",
    "   - Dropout: OFF (use all neurons)\n",
    "   - Batch Normalization: Use running stats (don't update)\n",
    "\n",
    "**Why separate validation?**\n",
    "- Measure generalization (how well model works on unseen data)\n",
    "- Detect overfitting (train loss ‚Üì, val loss ‚Üë)\n",
    "- Unbiased performance estimate\n",
    "\n",
    "### The Training Loop Pattern\n",
    "\n",
    "```\n",
    "for epoch in range(50):\n",
    "    # Training phase\n",
    "    model.train()  # Enable dropout\n",
    "    train_loss = train_fn(...)  # Update weights\n",
    "    \n",
    "    # Validation phase\n",
    "    model.eval()  # Disable dropout\n",
    "    val_loss = eval_fn(...)  # No weight updates\n",
    "    \n",
    "    # Learning rate adjustment\n",
    "    scheduler.step(val_loss)  # Reduce LR if stuck\n",
    "```\n",
    "\n",
    "**Output Interpretation:**\n",
    "```\n",
    "Epoch 1: Train Loss: 3.2 | Val Loss: 3.5  ‚Üê High loss, random guessing\n",
    "Epoch 10: Train Loss: 0.8 | Val Loss: 1.2  ‚Üê Getting better\n",
    "Epoch 30: Train Loss: 0.1 | Val Loss: 0.3  ‚Üê Good! Close losses\n",
    "Epoch 40: Train Loss: 0.01 | Val Loss: 1.5  ‚Üê Overfitting! Val loss increased\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d09da91",
   "metadata": {},
   "source": [
    "## 6. Main Training Pipeline\n",
    "\n",
    "**Data Preparation:**\n",
    "1. **Load images** - Get all .jpg files from train/val directories\n",
    "2. **Extract labels** - Filename is the captcha text (e.g., \"8AE5T.jpg\" ‚Üí \"8AE5T\")\n",
    "3. **Split into characters** - \"8AE5T\" ‚Üí ['8', 'A', 'E', '5', 'T']\n",
    "4. **Encode characters** - Fit LabelEncoder on all unique characters\n",
    "5. **Add +1 offset** - Reserve 0 for CTC blank token\n",
    "\n",
    "**Example encoding:**\n",
    "```\n",
    "Original: \"8AE5T\"\n",
    "Split: ['8', 'A', 'E', '5', 'T']\n",
    "Encode: [8, 10, 4, 5, 19] (example indices from LabelEncoder)\n",
    "Add +1: [9, 11, 5, 6, 20] (reserve 0 for blank)\n",
    "```\n",
    "\n",
    "**Training Loop:**\n",
    "- Train on all batches (forward + backward pass)\n",
    "- Validate on test set (forward only)\n",
    "- Reduce learning rate if validation loss plateaus\n",
    "- Save best model checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cdac4de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "def run_training():\n",
    "    # Load training and validation image paths\n",
    "    train_images = glob.glob(\"Dataset/train/*.jpg\")\n",
    "    val_images = glob.glob(\"Dataset/val/*.jpg\")\n",
    "    \n",
    "    # Extract labels from filenames (e.g., \"Dataset/train/8AE5T.jpg\" -> \"8AE5T\")\n",
    "    train_labels = [x.split(os.sep)[-1].split('.')[0] for x in train_images]\n",
    "    val_labels = [x.split(os.sep)[-1].split('.')[0] for x in val_images]\n",
    "    \n",
    "    # Split strings into individual characters for CTC\n",
    "    train_targets = [[c for c in x] for x in train_labels]\n",
    "    val_targets = [[c for c in x] for x in val_labels]\n",
    "    \n",
    "    # Flatten all characters for LabelEncoder\n",
    "    targets_flat = [c for clist in train_targets for c in clist]\n",
    "    targets_flat += [c for clist in val_targets for c in clist]\n",
    "    \n",
    "    # Fit encoder on all unique characters\n",
    "    lbl_enc = LabelEncoder()\n",
    "    lbl_enc.fit(targets_flat)\n",
    "    \n",
    "    # Encode labels to integers\n",
    "    train_encoded = [lbl_enc.transform(x) for x in train_targets]\n",
    "    val_encoded = [lbl_enc.transform(x) for x in val_targets]\n",
    "    \n",
    "    # Add +1 to reserve 0 for CTC blank token\n",
    "    train_encoded = [[i + 1 for i in x] for x in train_encoded]\n",
    "    val_encoded = [[i + 1 for i in x] for x in val_encoded]\n",
    "    \n",
    "    # Create datasets\n",
    "    train_dataset = ClassificationDataset(train_images, train_encoded)\n",
    "    val_dataset = ClassificationDataset(val_images, val_encoded)\n",
    "    \n",
    "    # Create dataloaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=4)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=4)\n",
    "    \n",
    "    # Initialize model\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    model = CaptchaModel(num_characters=len(lbl_enc.classes_))\n",
    "    model.to(device)\n",
    "    \n",
    "    # Setup optimizer and scheduler\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, factor=0.5, patience=5, verbose=True\n",
    "    )\n",
    "    \n",
    "    # Training loop\n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    \n",
    "    for epoch in range(50):\n",
    "        train_loss = train_fn(model, train_loader, optimizer, device)\n",
    "        val_loss = eval_fn(model, val_loader, device)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/50 - Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n",
    "        \n",
    "        scheduler.step(val_loss)\n",
    "        \n",
    "        # Save best model\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), 'best_captcha_model.pth')\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "        \n",
    "        # Early stopping\n",
    "        if patience_counter >= 10:\n",
    "            print(\"Early stopping triggered!\")\n",
    "            break\n",
    "    \n",
    "    # Save label encoder\n",
    "    with open('label_encoder.pkl', 'wb') as f:\n",
    "        pickle.dump(lbl_enc, f)\n",
    "    \n",
    "    print(\"\\nTraining complete!\")\n",
    "    return model, lbl_enc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dc30536",
   "metadata": {},
   "source": [
    "## üìö Training Function - Deep Dive\n",
    "\n",
    "### Character Encoding Process (Most Critical Part!)\n",
    "\n",
    "**Problem:** Model needs numbers, but we have characters like \"8AE5T\"\n",
    "\n",
    "**Solution: Multi-Step Encoding**\n",
    "\n",
    "#### Step 1: Load Image Paths\n",
    "```python\n",
    "train_images = glob.glob(\"Dataset/train/*.jpg\")\n",
    "# Result: [\"Dataset/train/8AE5T.jpg\", \"Dataset/train/BC3X2.jpg\", ...]\n",
    "```\n",
    "\n",
    "#### Step 2: Extract Labels from Filenames\n",
    "```python\n",
    "train_labels = [x.split(os.sep)[-1].split('.')[0] for x in train_images]\n",
    "# Process:\n",
    "#   \"Dataset/train/8AE5T.jpg\"\n",
    "#   ‚Üì .split(os.sep) ‚Üí [\"Dataset\", \"train\", \"8AE5T.jpg\"]\n",
    "#   ‚Üì [-1] ‚Üí \"8AE5T.jpg\"\n",
    "#   ‚Üì .split('.') ‚Üí [\"8AE5T\", \"jpg\"]\n",
    "#   ‚Üì [0] ‚Üí \"8AE5T\"\n",
    "# Result: [\"8AE5T\", \"BC3X2\", \"K7M9P\", ...]\n",
    "```\n",
    "\n",
    "#### Step 3: Split into Individual Characters\n",
    "```python\n",
    "train_targets = [[c for c in x] for x in train_labels]\n",
    "# Process:\n",
    "#   \"8AE5T\" ‚Üí ['8', 'A', 'E', '5', 'T']\n",
    "#   \"BC3X2\" ‚Üí ['B', 'C', '3', 'X', '2']\n",
    "# Result: [['8','A','E','5','T'], ['B','C','3','X','2'], ...]\n",
    "```\n",
    "\n",
    "#### Step 4: Flatten All Characters (Critical for LabelEncoder)\n",
    "```python\n",
    "targets_flat = [c for clist in train_targets for c in clist]\n",
    "# Process (nested list comprehension):\n",
    "#   For each list: ['8','A','E','5','T']\n",
    "#   For each character in list: '8', 'A', 'E', '5', 'T'\n",
    "#   Collect all into single list\n",
    "# Result: ['8','A','E','5','T','B','C','3','X','2','K','7','M','9','P',...]\n",
    "```\n",
    "**Why flatten?** LabelEncoder.fit() needs ALL unique characters to create mapping.\n",
    "\n",
    "#### Step 5: Fit LabelEncoder\n",
    "```python\n",
    "lbl_enc.fit(targets_flat)\n",
    "# Creates mapping:\n",
    "# '0'‚Üí0, '1'‚Üí1, ..., '9'‚Üí9, 'A'‚Üí10, 'B'‚Üí11, ..., 'Z'‚Üí35\n",
    "# Total: 36 classes (0-9, A-Z)\n",
    "```\n",
    "\n",
    "#### Step 6: Encode Each Character List\n",
    "```python\n",
    "train_encoded = [lbl_enc.transform(x) for x in train_targets]\n",
    "# Process:\n",
    "#   ['8','A','E','5','T'] ‚Üí [8, 10, 14, 5, 19]\n",
    "#   'A' maps to 10, 'E' to 14, etc.\n",
    "# Result: [[8,10,14,5,19], [11,12,3,23,2], ...]\n",
    "```\n",
    "\n",
    "#### Step 7: Add +1 Offset (CTC Requirement)\n",
    "```python\n",
    "train_encoded = [[i + 1 for i in x] for x in train_encoded]\n",
    "# Process:\n",
    "#   [8, 10, 14, 5, 19] ‚Üí [9, 11, 15, 6, 20]\n",
    "# Why? Reserve index 0 for CTC blank token\n",
    "# CTC uses 0 for alignment gaps: \"A__B\" ‚Üí [1, 0, 0, 2]\n",
    "```\n",
    "\n",
    "**Final Encoding Example:**\n",
    "```\n",
    "Original string: \"8AE5T\"\n",
    "Split:           ['8', 'A', 'E', '5', 'T']\n",
    "Encode:          [8, 10, 14, 5, 19]\n",
    "+1 Offset:       [9, 11, 15, 6, 20]  ‚Üê This goes to model!\n",
    "```\n",
    "\n",
    "### DataLoader Parameters\n",
    "\n",
    "**`batch_size=32`:** Process 32 images simultaneously\n",
    "- GPU parallelization: Faster than processing one-by-one\n",
    "- Memory tradeoff: Larger batch = more GPU memory\n",
    "\n",
    "**`shuffle=True` (training):** Randomize sample order each epoch\n",
    "- Prevents model from learning order patterns\n",
    "- Better generalization\n",
    "\n",
    "**`shuffle=False` (validation):** Keep order consistent\n",
    "- Reproducible results\n",
    "- Order doesn't matter for evaluation\n",
    "\n",
    "**`num_workers=4`:** 4 parallel processes load data\n",
    "- Main process trains model\n",
    "- Worker processes load and preprocess images\n",
    "- Reduces data loading bottleneck"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27b07fc7",
   "metadata": {},
   "source": [
    "## 7. Prediction Functions\n",
    "\n",
    "**CTC Decoding:** Convert model output to text\n",
    "\n",
    "Model outputs: `[11, 11, 0, 12, 12, 12, 0, 13, 13, 0]`\n",
    "- Index 0 is CTC blank (ignore)\n",
    "- Remove consecutive duplicates: `[11, 12, 13]`\n",
    "- Subtract 1 to get original indices: `[10, 11, 12]`\n",
    "- Convert to characters: `['A', 'B', 'C']`\n",
    "- Final text: `\"ABC\"`\n",
    "\n",
    "**Functions:**\n",
    "- `remove_duplicates()` - Remove consecutive same characters\n",
    "- `decode_predictions()` - Full CTC decoding\n",
    "- `predict_single_image()` - Predict one image\n",
    "- `predict_batch()` - Predict multiple images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92a8f111",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_duplicates(x):\n",
    "    if len(x) < 2:\n",
    "        return x\n",
    "    fin = \"\"\n",
    "    for j in x:\n",
    "        if fin == \"\" or j != fin[-1]:\n",
    "            fin += j\n",
    "    return fin\n",
    "\n",
    "\n",
    "def decode_predictions(preds, encoder):\n",
    "    preds = preds.permute(1, 0, 2)  # (seq, batch, classes) -> (batch, seq, classes)\n",
    "    preds = torch.softmax(preds, 2)\n",
    "    preds = torch.argmax(preds, 2)\n",
    "    preds = preds.detach().cpu().numpy()\n",
    "    \n",
    "    cap_preds = []\n",
    "    for j in range(preds.shape[0]):\n",
    "        temp = []\n",
    "        for k in preds[j, :]:\n",
    "            k = k - 1  # Reverse +1 offset\n",
    "            if k == -1:\n",
    "                temp.append(\"¬ß\")  # Blank token placeholder\n",
    "            else:\n",
    "                temp.append(encoder.inverse_transform([k])[0])\n",
    "        \n",
    "        tp = \"\".join(temp)\n",
    "        tp = tp.replace(\"¬ß\", \"\")  # Remove blanks\n",
    "        cap_preds.append(remove_duplicates(tp))  # Remove duplicates\n",
    "    \n",
    "    return cap_preds\n",
    "\n",
    "\n",
    "def load_model(model_path, lbl_enc, device='cpu'):\n",
    "    model = CaptchaModel(num_characters=len(lbl_enc.classes_))\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "\n",
    "def predict_single_image(image_path, model, lbl_enc, device='cpu'):\n",
    "    dataset = ClassificationDataset([image_path], [[0, 0, 0, 0, 0]])\n",
    "    image_dict = dataset[0]\n",
    "    image_tensor = image_dict['images'].unsqueeze(0).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        preds, _ = model(image_tensor)\n",
    "    \n",
    "    text = decode_predictions(preds, lbl_enc)[0]\n",
    "    return text\n",
    "\n",
    "\n",
    "def predict_batch(image_paths, model, lbl_enc, device='cpu', batch_size=32):\n",
    "    dummy_labels = [[0, 0, 0, 0, 0] for _ in range(len(image_paths))]\n",
    "    dataset = ClassificationDataset(image_paths, dummy_labels)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "    \n",
    "    all_predictions = []\n",
    "    for data in tqdm(dataloader, desc=\"Predicting\"):\n",
    "        images = data['images'].to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            preds, _ = model(images)\n",
    "        \n",
    "        texts = decode_predictions(preds, lbl_enc)\n",
    "        all_predictions.extend(texts)\n",
    "    \n",
    "    return all_predictions\n",
    "\n",
    "\n",
    "def load_label_encoder(load_path='label_encoder.pkl'):\n",
    "    with open(load_path, 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "print(\"‚úì Prediction functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fecf8cf",
   "metadata": {},
   "source": [
    "## üîÆ CTC Decoding - Deep Dive\n",
    "\n",
    "### The Challenge\n",
    "Model outputs: `(37, batch, 37)` - 37 predictions per image, each with 37 probabilities\n",
    "We need: Simple text like \"ABC\"\n",
    "\n",
    "### `decode_predictions()` Function Breakdown\n",
    "\n",
    "**Input:**\n",
    "- `preds`: (seq_len=37, batch=32, num_classes=37) - Raw model output (logits)\n",
    "- `encoder`: LabelEncoder with character mappings\n",
    "\n",
    "**Process:**\n",
    "\n",
    "#### Step 1: Rearrange Dimensions\n",
    "```python\n",
    "preds = preds.permute(1, 0, 2)\n",
    "# (37, 32, 37) ‚Üí (32, 37, 37)\n",
    "# Why? Easier to loop through batch (32 images)\n",
    "```\n",
    "\n",
    "#### Step 2: Convert Logits to Probabilities\n",
    "```python\n",
    "preds = torch.softmax(preds, 2)\n",
    "# Before: [2.3, -1.5, 0.8, ...] (logits, any value)\n",
    "# After:  [0.65, 0.05, 0.15, ...] (probabilities, sum=1.0)\n",
    "# Why? Makes prediction confidence interpretable\n",
    "```\n",
    "\n",
    "#### Step 3: Get Most Likely Character\n",
    "```python\n",
    "preds = torch.argmax(preds, 2)\n",
    "# (32, 37, 37) ‚Üí (32, 37)\n",
    "# Each of 37 positions now has single index (0-36)\n",
    "# Example row: [11, 11, 0, 12, 12, 12, 0, 13, 13, 0, ...]\n",
    "```\n",
    "\n",
    "#### Step 4: Reverse the +1 Offset\n",
    "```python\n",
    "k = k - 1\n",
    "# Remember training? We added +1 to reserve 0 for blank\n",
    "# Now reverse it:\n",
    "#   Model output: 11 ‚Üí 11-1 = 10 ‚Üí maps to 'A'\n",
    "#   Model output: 0 ‚Üí 0-1 = -1 ‚Üí blank token\n",
    "```\n",
    "\n",
    "#### Step 5: Handle Blank Tokens\n",
    "```python\n",
    "if k == -1:\n",
    "    temp.append(\"¬ß\")  # Placeholder\n",
    "else:\n",
    "    temp.append(encoder.inverse_transform([k])[0])\n",
    "\n",
    "# Example sequence:\n",
    "#   [10, 10, -1, 11, 11, 11, -1, 12, 12, -1]\n",
    "#   ‚Üì\n",
    "#   ['A', 'A', '¬ß', 'B', 'B', 'B', '¬ß', 'C', 'C', '¬ß']\n",
    "```\n",
    "\n",
    "#### Step 6: Remove Blank Tokens\n",
    "```python\n",
    "tp = tp.replace(\"¬ß\", \"\")\n",
    "# \"A A ¬ß B B B ¬ß C C ¬ß\" ‚Üí \"AABBBCC\"\n",
    "```\n",
    "\n",
    "#### Step 7: Remove Consecutive Duplicates\n",
    "```python\n",
    "cap_preds.append(remove_duplicates(tp))\n",
    "# \"AABBBCC\" ‚Üí \"ABC\"\n",
    "# Why? CTC can output same character multiple times\n",
    "# Real example: \"HHEELLOO\" ‚Üí \"HELO\"\n",
    "```\n",
    "\n",
    "**Complete Example:**\n",
    "\n",
    "```\n",
    "Raw Model Output (37 positions):\n",
    "[11, 11, 11, 0, 0, 12, 12, 0, 13, 13, 13, 0, 0, 0, ...]\n",
    "\n",
    "Step 1: Subtract 1\n",
    "[10, 10, 10, -1, -1, 11, 11, -1, 12, 12, 12, -1, -1, -1, ...]\n",
    "\n",
    "Step 2: Map to characters (using LabelEncoder)\n",
    "['A', 'A', 'A', '¬ß', '¬ß', 'B', 'B', '¬ß', 'C', 'C', 'C', '¬ß', ...]\n",
    "\n",
    "Step 3: Remove blanks (¬ß)\n",
    "['A', 'A', 'A', 'B', 'B', 'C', 'C', 'C']\n",
    "\n",
    "Step 4: Remove consecutive duplicates\n",
    "['A', 'B', 'C']\n",
    "\n",
    "Step 5: Join to string\n",
    "\"ABC\"\n",
    "```\n",
    "\n",
    "**Output:**\n",
    "- List of decoded strings: `[\"ABC\", \"X7Y\", \"K2M9\", ...]`\n",
    "- One prediction per batch sample\n",
    "\n",
    "### Why CTC Works This Way\n",
    "\n",
    "**Problem:** We don't know exactly where each character appears in the image\n",
    "- \"ABC\" might span pixels 10-50, 60-100, 120-150\n",
    "- Different fonts/sizes cause different positions\n",
    "\n",
    "**CTC Solution:** \n",
    "- Output predictions for EVERY position (37 times)\n",
    "- Allow blanks and duplicates for alignment\n",
    "- Collapse to final text during decoding\n",
    "\n",
    "**Example Alignment:**\n",
    "```\n",
    "Image positions: |A|A|A|_|_|B|B|_|C|C|C|_|_|_|...\n",
    "Decoded text:    \"ABC\"\n",
    "```\n",
    "\n",
    "This flexibility allows the model to handle:\n",
    "- Variable character widths\n",
    "- Different spacing\n",
    "- Overlapping characters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4186c6c",
   "metadata": {},
   "source": [
    "## 8. Run Training\n",
    "\n",
    "Execute the complete training pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce851081",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "trained_model, label_encoder = run_training()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b78dc84",
   "metadata": {},
   "source": [
    "## 9. Test Predictions\n",
    "\n",
    "Test the trained model on validation images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25f3649a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load saved model and label encoder\n",
    "lbl_enc = load_label_encoder('label_encoder.pkl')\n",
    "model = load_model('best_captcha_model.pth', lbl_enc, device='cuda')\n",
    "\n",
    "print(\"Model loaded successfully!\")\n",
    "print(f\"Number of character classes: {len(lbl_enc.classes_)}\")\n",
    "print(f\"Characters: {list(lbl_enc.classes_)}\")\n",
    "\n",
    "# Get test images\n",
    "val_images = glob.glob(\"Dataset/val/*.jpg\")\n",
    "test_images = val_images[:5]\n",
    "\n",
    "print(f\"\\nTesting on {len(test_images)} images...\")\n",
    "\n",
    "# Predict\n",
    "predictions = predict_batch(test_images, model, lbl_enc, device='cuda', batch_size=5)\n",
    "\n",
    "# Display results\n",
    "print(\"\\nPrediction Results:\")\n",
    "print(\"‚îÄ\" * 60)\n",
    "for img_path, pred_text in zip(test_images, predictions):\n",
    "    true_label = img_path.split(os.sep)[-1].split('.')[0]\n",
    "    status = \"‚úì\" if pred_text == true_label else \"‚úó\"\n",
    "    print(f\"{status} {img_path.split(os.sep)[-1]:15s} | True: {true_label} | Pred: {pred_text}\")\n",
    "print(\"‚îÄ\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45cd3ead",
   "metadata": {},
   "source": [
    "## üß™ Testing the Model\n",
    "\n",
    "Let's test our trained model on validation images and see the predictions!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "493df73c",
   "metadata": {},
   "source": [
    "## 10. Visualize Predictions\n",
    "\n",
    "Display sample predictions with images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1b0ffa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "# Visualize predictions\n",
    "fig, axes = plt.subplots(1, 5, figsize=(15, 3))\n",
    "\n",
    "for idx, (img_path, pred_text) in enumerate(zip(test_images, predictions)):\n",
    "    img = Image.open(img_path).convert('RGB')\n",
    "    true_label = img_path.split(os.sep)[-1].split('.')[0]\n",
    "    \n",
    "    axes[idx].imshow(img)\n",
    "    color = 'green' if pred_text == true_label else 'red'\n",
    "    axes[idx].set_title(f\"Pred: {pred_text}\\nTrue: {true_label}\", color=color, fontsize=10)\n",
    "    axes[idx].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calculate validation accuracy\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"VALIDATION SET ACCURACY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "all_val_predictions = predict_batch(val_images, model, lbl_enc, device='cuda', batch_size=32)\n",
    "\n",
    "correct = 0\n",
    "total = len(val_images)\n",
    "\n",
    "for img_path, pred in zip(val_images, all_val_predictions):\n",
    "    true_label = img_path.split(os.sep)[-1].split('.')[0]\n",
    "    if pred == true_label:\n",
    "        correct += 1\n",
    "\n",
    "accuracy = (correct / total) * 100\n",
    "\n",
    "print(f\"Total Images:        {total}\")\n",
    "print(f\"Correct Predictions: {correct}\")\n",
    "print(f\"Wrong Predictions:   {total - correct}\")\n",
    "print(f\"Accuracy:            {accuracy:.2f}%\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Character-level accuracy\n",
    "print(\"\\nCHARACTER-LEVEL ACCURACY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "total_chars = 0\n",
    "correct_chars = 0\n",
    "\n",
    "for img_path, pred in zip(val_images, all_val_predictions):\n",
    "    true_label = img_path.split(os.sep)[-1].split('.')[0]\n",
    "    \n",
    "    for i in range(min(len(pred), len(true_label))):\n",
    "        total_chars += 1\n",
    "        if i < len(pred) and i < len(true_label) and pred[i] == true_label[i]:\n",
    "            correct_chars += 1\n",
    "    \n",
    "    total_chars += abs(len(pred) - len(true_label))\n",
    "\n",
    "char_accuracy = (correct_chars / total_chars) * 100\n",
    "\n",
    "print(f\"Total Characters:    {total_chars}\")\n",
    "print(f\"Correct Characters:  {correct_chars}\")\n",
    "print(f\"Character Accuracy:  {char_accuracy:.2f}%\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\n‚úì Visualization and evaluation complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48da6576",
   "metadata": {},
   "source": [
    "## üìä Visualization & Accuracy Metrics\n",
    "\n",
    "Let's visualize our predictions and calculate overall accuracy on the validation set.\n",
    "\n",
    "**Metrics:**\n",
    "- **Image-level accuracy**: Entire CAPTCHA must be correct\n",
    "- **Character-level accuracy**: Individual character accuracy (more granular)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c82465ad",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### What We Built:\n",
    "1. **Dataset pipeline** - Load, preprocess, and batch images\n",
    "2. **CNN + GRU model** - Feature extraction + sequence modeling\n",
    "3. **CTC Loss training** - No character alignment needed\n",
    "4. **Prediction system** - Decode model output to text\n",
    "\n",
    "### Key Concepts:\n",
    "- **Character-level encoding** - Model predicts individual characters\n",
    "- **CTC Loss** - Handles variable-length sequences\n",
    "- **+1 offset** - Reserve index 0 for CTC blank token\n",
    "- **Bidirectional GRU** - Context from both directions\n",
    "\n",
    "### Files Generated:\n",
    "- `best_captcha_model.pth` - Trained model weights\n",
    "- `label_encoder.pkl` - Character encoder for prediction\n",
    "\n",
    "### Next Steps:\n",
    "- Try data augmentation (rotation, noise)\n",
    "- Experiment with different architectures\n",
    "- Add beam search decoding for better accuracy\n",
    "- Deploy as web API"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
